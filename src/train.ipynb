{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THESE LINES.\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from feature_engine.encoding import OneHotEncoder\n",
    "from joblib import dump\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT CHANGE THESE LINES.\n",
    "ROOT_DIR = os.path.dirname(os.getcwd())\n",
    "MODEL_INPUTS_OUTPUTS = os.path.join(ROOT_DIR, 'model_inputs_outputs/')\n",
    "INPUT_DIR = os.path.join(MODEL_INPUTS_OUTPUTS, \"inputs\")\n",
    "INPUT_SCHEMA_DIR = os.path.join(INPUT_DIR, \"schema\")\n",
    "DATA_DIR = os.path.join(INPUT_DIR, \"data\")\n",
    "TRAIN_DIR = os.path.join(DATA_DIR, \"training\")\n",
    "TEST_DIR = os.path.join(DATA_DIR, \"testing\")\n",
    "MODEL_PATH = os.path.join(MODEL_INPUTS_OUTPUTS, \"model\")\n",
    "MODEL_ARTIFACTS_PATH = os.path.join(MODEL_PATH, \"artifacts\")\n",
    "OHE_ENCODER_FILE = os.path.join(MODEL_ARTIFACTS_PATH, 'ohe.joblib')\n",
    "PREDICTOR_DIR_PATH = os.path.join(MODEL_ARTIFACTS_PATH, \"predictor\")\n",
    "PREDICTOR_FILE_PATH = os.path.join(PREDICTOR_DIR_PATH, \"predictor.joblib\")\n",
    "IMPUTATION_FILE = os.path.join(MODEL_ARTIFACTS_PATH, 'imputation.joblib')\n",
    "LABEL_ENCODER_FILE = os.path.join(MODEL_ARTIFACTS_PATH, 'label_encoder.joblib')\n",
    "if not os.path.exists(MODEL_ARTIFACTS_PATH):\n",
    "    os.makedirs(MODEL_ARTIFACTS_PATH)\n",
    "if not os.path.exists(PREDICTOR_DIR_PATH):\n",
    "    os.makedirs(PREDICTOR_DIR_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the schema\n",
    "The schema contains metadata about the datasets. We will use the scehma to get information about the type of each feature (NUMERIC or CATEGORICAL) and the id and target features, this will be helpful in preprocessing stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = [f for f in os.listdir(INPUT_SCHEMA_DIR) if f.endswith('json')][0]\n",
    "schema_path = os.path.join(INPUT_SCHEMA_DIR, file_name)\n",
    "with open(schema_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    schema = json.load(file)\n",
    "features = schema['features']\n",
    "\n",
    "numeric_features = []\n",
    "categorical_features = []\n",
    "nullable_features = []\n",
    "for f in features:\n",
    "    if f['dataType'] == 'CATEGORICAL':\n",
    "        categorical_features.append(f['name'])\n",
    "    else:\n",
    "        numeric_features.append(f['name'])\n",
    "    if f['nullable']:\n",
    "        nullable_features.append(f['name'])\n",
    "\n",
    "id_feature = schema['id']['name']\n",
    "target_feature = schema['target']['name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unit_id</th>\n",
       "      <th>act_combined_midrange</th>\n",
       "      <th>act_english_midrange</th>\n",
       "      <th>act_math_midrange</th>\n",
       "      <th>act_writing_midrange</th>\n",
       "      <th>admission_rate</th>\n",
       "      <th>agege24</th>\n",
       "      <th>average_cost_academic_year</th>\n",
       "      <th>average_cost_program_year</th>\n",
       "      <th>carnegie_basic_classification</th>\n",
       "      <th>...</th>\n",
       "      <th>sat_math_midrange</th>\n",
       "      <th>sat_total_average</th>\n",
       "      <th>sat_verbal_midrange</th>\n",
       "      <th>sat_writing_midrange</th>\n",
       "      <th>spend_per_student</th>\n",
       "      <th>state</th>\n",
       "      <th>tuition_(instate)</th>\n",
       "      <th>tuition_(out_of_state)</th>\n",
       "      <th>undergrad_size</th>\n",
       "      <th>percent_pell_grant</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>459523</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9083.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4702.0</td>\n",
       "      <td>TX</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.8857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>123341</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.48</td>\n",
       "      <td>13157.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Associate\\s--Public Suburban-serving Multicampus'</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3674.0</td>\n",
       "      <td>CA</td>\n",
       "      <td>1150.0</td>\n",
       "      <td>6190.0</td>\n",
       "      <td>17195.0</td>\n",
       "      <td>0.3146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>172495</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.46</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17013.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>MI</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>111.0</td>\n",
       "      <td>0.6520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>419457</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.62</td>\n",
       "      <td>23317.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Special Focus Institutions--Other health profe...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6234.0</td>\n",
       "      <td>VA</td>\n",
       "      <td>13663.0</td>\n",
       "      <td>13663.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>0.5495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>455725</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15564.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3366.0</td>\n",
       "      <td>AZ</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>363.0</td>\n",
       "      <td>0.8227</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   unit_id  act_combined_midrange  act_english_midrange  act_math_midrange  \\\n",
       "0   459523                    NaN                   NaN                NaN   \n",
       "1   123341                    NaN                   NaN                NaN   \n",
       "2   172495                    NaN                   NaN                NaN   \n",
       "3   419457                    NaN                   NaN                NaN   \n",
       "4   455725                    NaN                   NaN                NaN   \n",
       "\n",
       "   act_writing_midrange  admission_rate  agege24  average_cost_academic_year  \\\n",
       "0                   NaN             NaN      NaN                         NaN   \n",
       "1                   NaN             NaN     0.48                     13157.0   \n",
       "2                   NaN             NaN     0.46                         NaN   \n",
       "3                   NaN             NaN     0.62                     23317.0   \n",
       "4                   NaN             NaN      NaN                         NaN   \n",
       "\n",
       "   average_cost_program_year  \\\n",
       "0                     9083.0   \n",
       "1                        NaN   \n",
       "2                    17013.0   \n",
       "3                        NaN   \n",
       "4                    15564.0   \n",
       "\n",
       "                       carnegie_basic_classification  ... sat_math_midrange  \\\n",
       "0                                                NaN  ...               NaN   \n",
       "1  Associate\\s--Public Suburban-serving Multicampus'  ...               NaN   \n",
       "2                                                NaN  ...               NaN   \n",
       "3  Special Focus Institutions--Other health profe...  ...               NaN   \n",
       "4                                                NaN  ...               NaN   \n",
       "\n",
       "  sat_total_average  sat_verbal_midrange  sat_writing_midrange  \\\n",
       "0               NaN                  NaN                   NaN   \n",
       "1               NaN                  NaN                   NaN   \n",
       "2               NaN                  NaN                   NaN   \n",
       "3               NaN                  NaN                   NaN   \n",
       "4               NaN                  NaN                   NaN   \n",
       "\n",
       "   spend_per_student state tuition_(instate)  tuition_(out_of_state)  \\\n",
       "0             4702.0    TX               NaN                     NaN   \n",
       "1             3674.0    CA            1150.0                  6190.0   \n",
       "2             2015.0    MI               NaN                     NaN   \n",
       "3             6234.0    VA           13663.0                 13663.0   \n",
       "4             3366.0    AZ               NaN                     NaN   \n",
       "\n",
       "   undergrad_size  percent_pell_grant  \n",
       "0            35.0              0.8857  \n",
       "1         17195.0              0.3146  \n",
       "2           111.0              0.6520  \n",
       "3           126.0              0.5495  \n",
       "4           363.0              0.8227  \n",
       "\n",
       "[5 rows x 42 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_name = [f for f in os.listdir(TRAIN_DIR) if f.endswith('.csv')][0]\n",
    "file_path = os.path.join(TRAIN_DIR, file_name)\n",
    "df = pd.read_csv(file_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "Data preprocessing is very important before training the model, as the data may contain missing values in some cells. Moreover, most of the learning algorithms cannot work with categorical data, thus the data has to be encoded.\n",
    "\n",
    "In this section we will impute the missing values and encode the categorical features. Afterwards the data will be ready to train the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Imputing missing data\n",
    "> The median value will be used to impute missing values of the numeric features and the mode will be used to impute categorical features.\n",
    "\n",
    "##### You can add your own preprocessing steps such as:\n",
    "<ul>\n",
    "<li>Normalization</li> <br>\n",
    "<li>Outlier removal</li><br>\n",
    "<li>Handling imbalanced classes</li><br>\n",
    "<li>Dropping or adding features</li><br>\n",
    "</ul>\n",
    "\n",
    "### Important note:\n",
    "<p> \n",
    "Saving the values used for imputation during training step is crucial. These values will be used to impute missing data in the testing set. This is very important to avoid the well known problem of data leakage. During testing, you should not make any assumptions about the data in hand, alternatively anything needed during the testing phase should be learned from the training phase. This is why we are creating a dictionary of values used during training to reuse these values during testing.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/moo/Desktop/Upwork/rt-ML/Jupyter-Notebook-Python-Regression-Template/model_inputs_outputs/model/artifacts/imputation.joblib']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imputing missing data\n",
    "imputation_values = {}\n",
    "for column in nullable_features:    \n",
    "    if column in numeric_features:\n",
    "        value = df[column].median()\n",
    "    else:\n",
    "        value = df[column].mode()[0]\n",
    "    df[column].fillna(value, inplace=True)\n",
    "    imputation_values[column] = value\n",
    "\n",
    "dump(imputation_values, IMPUTATION_FILE)\n",
    "\n",
    "# Comment the above code and write you own imputation code here\n",
    "\n",
    "\n",
    "#BEGIN\n",
    "\n",
    "#CODE HERE\n",
    "\n",
    "#END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Encoding Categorical features\n",
    "<p>\n",
    "The id column is just an identifier for the training example, so we will exclude it during the encoding phase.<br>\n",
    "Target feature will be label encoded in the next step.\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the id and target columns in a different variable.\n",
    "ids = df[id_feature]\n",
    "target = df[target_feature]\n",
    "\n",
    "# Dropping the id and target from the dataframe\n",
    "df.drop(columns=[id_feature, target_feature], inplace=True)\n",
    "\n",
    "# Ensure that all categorical columns are stored as str type.\n",
    "# This is to ensure that even if the categories are numbers (1, 2, ...), they still get encoded.\n",
    "for c in categorical_features:\n",
    "    df[c] = df[c].astype(str)\n",
    "\n",
    "# Encoding the categorical features if exist\n",
    "if categorical_features:\n",
    "    encoder = OneHotEncoder(top_categories=6)\n",
    "    encoder.fit(df)\n",
    "    df = encoder.transform(df)\n",
    "\n",
    "    # Saving the encoder to use it on the testing dataset\n",
    "    path = dump(encoder, OHE_ENCODER_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Classifier\n",
    "We choose Linear Regression model, but feel free to try your own and compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a linear regression model and training it\n",
    "model = LinearRegression()\n",
    "model.fit(df, target)\n",
    "\n",
    "# BEGIN\n",
    "\n",
    "# model = ...\n",
    "\n",
    "# END\n",
    "\n",
    "# Saving the model to use it for predictions\n",
    "path = dump(model, PREDICTOR_FILE_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
